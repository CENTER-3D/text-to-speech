---

copyright:
  years: 2015, 2019
lastupdated: "2019-07-08"

subcollection: text-to-speech

---

{:shortdesc: .shortdesc}
{:external: target="_blank" .external}
{:tip: .tip}
{:important: .important}
{:note: .note}
{:deprecated: .deprecated}
{:pre: .pre}
{:codeblock: .codeblock}
{:screen: .screen}
{:javascript: .ph data-hd-programlang='javascript'}
{:java: .ph data-hd-programlang='java'}
{:python: .ph data-hd-programlang='python'}
{:swift: .ph data-hd-programlang='swift'}

# サービスの背景にある科学
{: #science}

{{site.data.keyword.texttospeechfull}} サービスは、2 つのタイプのテクノロジーに依存する音声を提供します。それらは、[ニューラル音声テクノロジー](#science-neural)と[連結的合成](#science-concatenative)です。どちらのテクノロジーも入力テキストから音声を合成しますが、それらは異なる手段を使用して異なる特性の音声を生成します。ニューラル (`V3`) 音声は、サービスの元となる連結的音声を拡張したバージョンです。
{: shortdesc}

テキストからの音声合成のトピックは本質的に複雑です。サービスの音声テクノロジーを支えている科学的研究について詳しくは、[研究資料](/docs/services/text-to-speech?topic=text-to-speech-references)にリストしている資料を参照してください。

## ニューラル音声テクノロジー
{: #science-neural}

ニューラル音声テクノロジーは、入力テキストから人間のクオリティーの音声を合成します。サービスは最初に、入力テキストを分析して目的のコンテンツを判別します。連結的モデルの場合と同様に、このサービスは、合成の候補ユニットを生成するデシジョン・ツリーから構成される音響モデルを使用します。

合成する音のシーケンスに含まれる 1 音ごとに、モデルは、その音の直前および直後の 2 つの音のコンテキストを考慮します。 その後、適合性が評価される音響ユニットのセットを生成します。このステップで、あるコンテキスト基準を満たすユニットのみに検索を制限し、他のすべてを破棄することによって、検索の複雑さが軽減されます。

その後、サービスは以下の 3 つのディープ・ニューラル・ネットワーク (DNN) を使用して、音声の音響 (スペクトル) 特性を予測し、生成される音声をエンコードします。

-   韻律予測
-   音響特性予測
-   ニューラル・ボコーダー

合成の際に、DNN はピッチと音素の持続時間 (韻律)、スペクトル構造、および音声の波形を予測します。例えば、韻律予測モジュールは、入力テキストから抽出される言語特性のターゲット値を生成します。その特性には、品詞、語彙強勢、単語レベルのプロミネンス、位置特性 (例えば、センテンス内の音節または単語の位置) などの属性が含まれます。

DNN は、音声の音響特性を予測できるように、自然な人間の音声を使用したトレーニングを受けています。このモジュラー・アプローチには、素早く簡単なトレーニングが可能になり、各コンポーネントを独立して制御できるという利点があります。基本のネットワークがトレーニングを受けた後、製品化と個別設定のために、それらを新しい発話スタイルや音声に適応させることができます。

ニューラル音声は、とても自然に聞こえて滑らかな音声品質を持つ、明瞭でクリアな音声を生成します。サービスのニューラル音声テクノロジーについて詳しくは、以下を参照してください。

-   ブログ投稿 [IBM Watson Text to Speech: Neural Voices Generally Available](https://medium.com/ibm-watson/ibm-watson-text-to-speech-neural-voices-added-to-service-e562106ff9c7){: external}
-   調査記事 [High quality, lightweight and adaptable Text to Speech using LPCNet](https://arxiv.org/abs/1905.00590){: external}

## 連結的合成
{: #science-concatenative}

連結的合成は、任意の入力テキストに対し、大規模な合成コーパスに由来する音響ユニットのインベントリーを基にして出力音声を生成します。これは、以下のプロセスで構成されるパイプラインに基づいています。 これらのプロセスにより、このユニットのインベントリーを効率的にリアルタイムで検索し、それに続いてユニットの後処理を行うことが簡単になります。

-   **音響モデル** - このモデルは、検索に対して候補ユニットを生成するデシジョン・ツリーから構成されます。 合成する音のシーケンスに含まれる 1 音ごとに、モデルは、その音の直前および直後の 2 つの音のコンテキストを考慮します。 その後、検索の適合性を評価する音響ユニットのセットを生成します。 このステップで、あるコンテキスト基準を満たすユニットのみに検索を制限し、他のすべてを破棄することによって、検索の複雑さが効果的に軽減されます。
-   **韻律ターゲット・モデル** - 一部の音声の韻律ターゲット・モデルは、Deep Recurrent Neural Networks (RNN) に基づいています。その他の音声の場合、モデルはデシジョン・ツリーに依存して韻律を予測します。どちらのケースでも、入力テキストから抽出された一連の言語特性を元に、モデルでは音声の韻律的な面 (期間やイントネーションなど) に対してターゲット値を生成します。このリストには、品詞、語彙強勢、単語レベルのプロミネンス、位置特性 (例えば、センテンス内の音節または単語の位置) などの属性が含まれます。 韻律ターゲット・モデルは、そのモデルによって予測された韻律基準を満たすユニットへと検索を導きます。
-   **検索** - 音響モデルおよびターゲット韻律から候補のリストが返されると、このモジュールは、Viterbi 検索を実行します。 この検索では、連結コストとターゲット・コストの両方を考慮するコスト関数が最小になる音響ユニットのシーケンスが抽出されます。 その結果、2 つのユニットを連結してできる音響成果物が最少化されます。また、このモジュールは、韻律ターゲット・モデルから提示されたターゲット韻律に近づけようとします。 この検索では、合成コーパス内の連続チャンクが優先的に選択されるため、そのような成果物がさらに削減されます。
-   **波形の生成** - 検索から最適なユニットのシーケンスが返されたら、システムは、時間領域の Pitch Synchronous Overlap and Add (PSOLA) を使用して、出力波形を生成します。 PSOLA は、音声処理に使用されるデジタル信号処理技法です。 特に音声合成に使用されます。 音声信号のピッチと持続時間を変更し、検索で返されたユニットをシームレスに一体化できます。

    前述のバックエンド処理における言語特性のすべてについて、サービスは、テキスト処理フロントエンドを使用してテキストを解析してから、音声形式に変換します。 このフロントエンドは、HTML タグなどのフォーマット設定成果物のテキストをサニタイズします。 次に、言語依存の言語学的ルールによって駆動される専有の言語を使用して、テキストを準備し、発音を生成します。 このモジュールは、日付、時間、数値、通貨など、テキストの言語依存の特性を正規化します。 例えば、辞書に基づいて略語を展開したり、序数および基数のルールを使用して数値を展開したりします。

    複数の発音が許容される単語もあるため、テキスト処理フロントエンドではまず、実行時に、単一の標準的発音を生成します。 この手法では、音声コーパスの記録時に話者が使用した発音が反映されない可能性があります。 そのため、サービスは、代替基本形式辞書に登録されている代替形式を使用して、発音候補のセットを拡張します。 ピッチ、持続時間、連続性の問題や制約の観点からコストが削減される形式を検索で選択できるようにします。 このアルゴリズムにより、データ・セットからより長い連続チャンクが選択されやすくなるため、合成結果の音声のフローが最適化されます。
